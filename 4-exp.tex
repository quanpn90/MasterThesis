\section{Experiment details}
We evaluate our model on two English corpora of different sizes and
genres that have been used for language modeling evaluation
before. The {\bf Penn Treebank} contains one million words of
newspaper text with 10K words in the vocabulary. We reuse the
preprocessing and train/test/valid division
from~\cite{mikolov2014learning}. 
%\gbt{Changed Europarl to Europarl-NC because Europarl usually refers
%to the multilingual Europarl corpus developed by I think Koehn ages
%ago.}
{\bf Europarl-NC} is a a 64-million word corpus that was developed for
a Machine Translation shared task~\cite{bojar-EtAl:2015:WMT},
combining Europarl data (from parliamentary debates in the European
Union) and News Commentary data.  We preprocessed the corpus with
tokenization and true-casing tools from the Moses
toolkit~\cite{koehn2007moses}. The vocabulary is composed of words
that occur at least 3 times in the training set and contains
approximately 60K words. %We use the validation and test set
% of the MT shared task.

We train our models using Stochastic Gradient Descent (SGD) and we
reduce the learning rate by a fixed proportion every time the
validation perplexity increases after one epoch. The values for learning rate,
learning rate shrinking 
%\gbt{isn't it ``learning rate decay''?}
and mini-batch sizes as well as context size are
fixed once and for all based on insights drawn from previous
work~\cite{le2011structured,sukhbaatar2015end,devlin2014fast} and
through experimentation with the Penn Treebank validation set. Experimentally we found SGD to be efficient and adequately fast, while other learning algorithms involve additional hyper parameters (such as alpha in RMSprop~\cite{tieleman2012lecture})

%For parameter tuning, we are concerned with trying the parameters which are less
%data-dependent including learning rate, learning rate shrink and mini-batch size on the small
%corpus (Penntreebank) and then recycle them on experiments with larger corpora. 
%The choice 
%of parameters also refers to previous works in feed-forward networks~\cite{le2011structured,sukhbaatar2015end,devlin2014fast}.
Specifically, the learning rate is set to $0.05$, with mini-batch size
of 128 (we do not take the average of loss over the batch, and the
training set is shuffled). We multiply the learning rate by $0.5$
every time we shrink it and clip the gradients if their norm is larger
than $12$. The network parameters are initialized randomly on a range
from $-0.01$ to $0.01$ and the context size is set to $16$. In
Section~\ref{sect:ana} we show that this large context window is fully
exploited. 
%The property of neural networks allow us to use such large context. We
%also aim at finding if the network can take into account the context words from afar. 

For the base FFNN and CNN we study embedding sizes (and thus, number of kernels) $k=128,256$. For $k=128$ we explore the simple CNN, incrementally adding NIN and COM variations (in that order) and, alternatively, using a ML-CNN. For $k=256$, we only explore the former three alternatives. For the kernel size, we set it to $w=3$ words for the simple CNN (out of options $3,5,7,9$), whereas for the COM variant we use $w=3$ and $5$. However, we observed the models to be generally robust to this parameter. Dropout rates are tuned specifically for each combination of model and dataset based on the validation perplexity. We also add small dropout ($0.05-0.15$) when we train the networks on the small corpus (Penn Treebank). 

% Data-dependent parameters are word embedding $k$ (the size of hidden layers is scaled by $k$),
%and dropout. For each corpus, we started with a low embedding size then increased it exponentially.
%The dropout value is tuned on the validation data. We report the test perplexity (lower is better) after getting the best
%dropout value for the embedding size. The size of the network is also restricted by our computational resource.

The experimental results for recurrent neural network language models,
such as Recurrent Neural Networks (RNN) and Long-Short Term Memory (LSTM), on the Penn Treebank are quoted from previous
work; for Europarl-NC, we train our own models (we also report the
performance of these in-house trained RNN and LSTM models on the Penn Treebank for
reference). Specifically, we train LSTMs with embedding size $k=256$
and number of layers $L=2$ as well as $k=512$ with $L=1,2$. We train
one RNN with $k=512$ and $L=2$. To train these models, we use the
published source code from~\cite{zaremba2014recurrent}. Our own models are also
implemented in Torch7\footnote{We'll link the source code here if the paper is accepted} %available at https://github.com/quanpn90/convlm.} 
	for easier comparison.

 For all models trained on Europarl-NC, we used hierarchical softmax to
speed up training, clustering the words in the vocabulary into 245
classes based on word frequency.

%~ \gbt{What vocabulary size?};
%~ {\bf Text8}, comprising the first 17 million words from a Wikipedia
%~ dump with about $44$K words in the vocabulary (also used
%~ in~\newcite{mikolov2014learning}); and a 64-million word corpus ({\bf
  %~ Europarl-NC}) combining \gbt{pls check} Europarl data (from
%~ parliamentary debates in the European Union) and News Commentary
%~ monolingual data \gbt{what kind of genre is that?} that was developed
%~ for a Machine Translation shared task~\cite{bojar-EtAl:2015:WMT}.

%~ For the first two corpora, we reuse the preprocessing steps
%~ in~\cite{mikolov2014learning}. For Europarl-NC, after preprocessing the
%~ corpus with tokenization and true-casing with tools provided in the
%~ Moses toolkit~\cite{koehn2007moses}, we selected the words that appear
%~ at least 3 times in the training set to obtain a vocabulary of
%~ approximately $60K$ words.

%~ To implement our models, we
%~ used Torch 7~\cite{collobert2011torch7} because of its fast CNN
%~ modules. We also used the implementation of LSTM/RNN language models
%~ in~\cite{zaremba2014recurrent} for the comparative experiments.

%~ As mentioned above, we fix the number of kernels in each convolutional
%~ layer to be equal to the embedding dimensionality $k$. Furthermore,
%~ the size of the first hidden layer --after the projection in the FFLM
%~ or after the convolutional layer in the CNN-- is also kept at
%~ $2 \times k$. We did not tune this parameter, but rather report
%~ results for different choices of $k$.

 %For example, \`baseline-128\' refers to the feed-forward model with embedding size of $128$ and hidden layer size of $256$. The upgraded CNN model has one more convolutional layer with $256$ feature maps. The second improvement comes from the Network-in-Network (``+Nin'') structure applied to the convolutional kernel. 
 %The kernel window size and padding parameters are adjusted so that the convolutional outputs have the same size as the embedding inputs, after simple reshaping.

%~ As mentioned above, we fix the number of kernels in each convolutional
%~ layer to be equal to the embedding dimensionality $k$. Furthermore,
%~ the size of the first hidden layer --after the projection in the FFLM
%~ or after the convolutional layer in the CNN-- is also kept at
%~ $2 \times k$. We did not tune this parameter, but rather report
%~ results for different choices of $k$.
%For example, \`baseline-128\' refers to the feed-forward model with embedding size of $128$ and hidden layer size of $256$. The upgraded CNN model has one more convolutional layer with $256$ feature maps. The second improvement comes from the Network-in-Network (``+Nin'') structure applied to the convolutional kernel. 
%The kernel window size and padding parameters are adjusted so that the convolutional outputs have the same size as the embedding inputs, after simple reshaping. 
%~ {\bf FIXME: In section 3.2 explain that the window is centered on the the input $i$ and padding vectors are added before and after the full context}
%~ Other parameters we explore are window size (${3, 5, 7}$), dropout
%~ probability values (from $0.1$ to $0.5$ \gbt{check} in steps of $0.1$), and the number of
%~ convolutional layers (${1. 2}$) \gbt{I don't understand
  %~ ${1. 2}$}. \gbt{check following} The weights of the network are
%~ randomly initialised from a uniform distribution of values between
%~ $-0.01$ and $0.01$ based on other works in
%~ FFLM~\cite{schwenk2007continuous}. For the convolutional weights, we
%~ found that the presence of Batch Normalisation reduces the effect of
%~ the initialisation method. We do not observe any difference between
%~ using the initialization just described (randomly drawn from a uniform
%~ distribution $-0.01 - 0.01$) and the MSR initialization
%~ method~\cite{he2015delving}.

%~ \gbt{Quan, has the move requested in the commented-out text below been done? but I think that the
%~ ReLU information does belong here}
% {\bf $<$begin move to Section 3$>$}  Notably, dropout is only applied to the fully connected components, thus having no effect on the convolutional layer. The activation function used in all experiments is ReLU. Not only can ReLU avoid gradient saturation, the activated neurons always have positive values, this facilitating the analysis of subsequent hidden layers. {\bf $<$/end move to Section 3$>$}

%~ Regarding the context size, we use $16$ words in the context for most
%~ of our experiments.  {\bf $<$begin move to Section 1 or 3$>$ (as
  %~ motivation for CNN)} Neural network language models, by their
%~ nature, can scale up the distribution for an arbitrary context size,
%~ which could be too sparse for count-based
%~ models~\cite{kneser1995improved}. It is notable, however, that the
%~ feed forward models in~\cite{hai2012measuring} do not benefit much
%~ from a context larger than $10$, yet the author only took into account
%~ the words in the current sentence \gbt{I don't understand from ``yet''
  %~ to here}.  {\bf $<$/end move to Section 1 or 3$>$} We empirically
%~ choose the size of each mini-batch (shuffled during training) as $128$
%~ {\bf how were the parameters calibrated?} and update the parameters
%~ with simple mini-batch gradient descent. The learning rate is
%~ initially set to $0.05$ and gradually decreased by $2$ \gbt{you mean
  %~ halved, right? -- ``decreased by $2$'' ambiguous} when the
%~ validation perplexity stops decreasing, following previous work for
%~ similar
%~ networks~\cite{le2011structured,sukhbaatar2015end,devlin2014fast} (we
%~ do not average the loss value over batches). Typically it takes
%~ approximately $30$ epochs for the models (including the baseline FFLM)
%~ to converge.

%~ The experimental results for Recurrent Neural Network language models
%~ are mostly quoted from previous works. We use the published source
%~ code from~\cite{zaremba2014recurrent} to produce the results
%~ for RNN and LSTM for the Europarl-NC corpus.

%~ As standard in language modeling, we report perplexity \gbt{brief
  %~ description, no need for formula.} (lower is better). 

\section{Results}

Our experimental results are summarized in Table~\ref{tab:result}. %From the baseline, we gradually add a convolutional layer (+CNN), then a Network-in-Network convolutional layer (+NIN) and finally we tried combining two convolutional blocks together (+COM). 

\begin{table*}[t]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		Model & $k$ & ker & \multicolumn{3}{c|}{Penn Treebank} & \multicolumn{3}{c|}{Europarl-NC} \\
		& & &  val & test & \#p & val & test & \#p \\
		
		
		\hline 
		Vanilla FFNN      & 128 & -   & 156 & 147 & 4.5M        & - & - & -   \\
		Baseline FFNN         & 128 & -   & 114 & 109 & 4.5M        & - & - & -   \\
		+CNN              & 128 & 3   & 108 & 102 & 4.5M        & - & - &  -  \\
		+MLPConv          & 128 & 3   & 102 &  97 & 4.5M        & - & - &  -  \\
		+MLPConv+COM      & 128 & 3+5 & 96  &  {\bf 92} & 8M         & - & - & -    \\
		+ML-CNN ($2$ layers) & 128 & 3   & 113 & 108 &  8M       & - & -  & -  \\
		+ML-CNN ($4$ layers) & 128 & 3   & 130 & 124 &  8M       & - & -  & -   \\
		
		
		\hline
		Vanilla FFNN     & 256 & -   & 161 &  152  &  8.2M       & - & - & -   \\
		Baseline FFNN        & 256 & -   & 110 &  105  & 8.2M        & 133 & 174 & 48M \\
		+CNN             & 256 & 3   & 104 &  98   &  8.3M      & 112 & 133 & 48M \\
		+MLPConv         & 256 & 3   & 97  &  93   &  8.3M      & 107 & {128} & 48M \\
		+MLPConv+COM     & 256 & 3+5 & 95  &  \textbf{91} & 18M  & 108 & \textbf{128} & 83M \\
		+MLPConv+COM     & 512 & 3+5 & 96  &  92 & 52M  & - & - & - \\
		
		
		\hline
		Model & hid & \#l & \multicolumn{3}{c|}{Penn Treebank} & \multicolumn{3}{c|}{Europarl-NC} \\
		& & &  val & test & \#p & val & test & \#p \\
		\hline 
		RNN~\cite{mikolov2014learning}     & 300  & 1 & 133 & 129  & 6M & - & - & \\
		LSTM~\cite{mikolov2014learning}    & 300  & 1 & 120 & 115  & 6.3M  & - & - & \\
		LSTM~\cite{zaremba2014recurrent}   & 1500 & 2 & 82  & \textbf{78} & 48M  & - & - & \\
		LSTM (trained in-house)            & 256  & 2 & 108 & 103  & 5.1M & 137 & 155 & 31M \\
		LSTM (trained in-house)            & 512  & 1 & 123 & 118  & 12M & 133 & 149 & 62M\\
		LSTM (trained in-house)            & 512  & 2 & 94  & 90   & 11M & 114 & \textbf{124}& 63M\\
		RNN  (trained in-house)            & 512  & 2 & 129 & 121  & 10M & 152 & 173 & 61M\\
		\hline
	\end{tabular}
	\caption{Results on Penn Treebank and Europarl-NC. Figure of merit is
		perplexity (lower is better). Legend: $k$: embedding size;
		\textit{ker}: kernel size; \textit{val}: results on validation
		data; \textit{test}: results on test data; \textit{\#p}: number of
		parameters; \textit{hid}: size of hidden layers; \textit{\#l}:
		number of layers.}
	\label{tab:result}
\end{table*}
%
%\paragraph{Comparison with the baseline.} 
First of all, we can see that even though the FFNN gives a very competitive performance, the addition of convolutional layers is clearly effective to increase it even further\footnote{In our experiments, increasing the number of fully connected layers is harmful. Two hidden layers with highway connections is the best setting we could find.}. 
Concretely, we observe a solid 13\% reduction of perplexity compared to the feed-forward network after using Network-in-Network in all setups for both corpora.  CNN alone yields a 6\% improvement, while MLPConv, in line with our expectations, adds another 5\% reduction in perplexity. A final (smaller) improvement comes from combining kernels of size 3 and 5, which can be attributed to a more expressive model that can learn patterns of $n$-grams of different sizes.%, although this confounds with having a higher capacity model. However, in our experience, lager models did not necessarily perform better. CNNs with large embedding sizes often over-fitted even to the large Europarl-NC corpus. We still need further studies to determine whether these CNNs can be trained at tohse larger settings.
In contrast to the successful two variants above, the multi-layer CNN
did not help in better capturing the regularities of text, but rather the opposite:
the more convolutional layers were stacked,
the worse the performance. This also stands in contrast to the
tradition of convolutional networks in Computer Vision, where using
very deep convolutional neural networks is key to having better
models. In text, deep convolution for text representation is rather
rare, and has only been applied in sentence
representation~\cite{Kalchbrenner2014conv}. We conjecture that the
reason why deep CNNs may not be so effective for text could be the
non-recursive nature of the textual data after convolution. In other
words, the convolution output for an image can be construed to be a new
image, which yet again can be subject to new convolution operations,
whereas the textual counterpart may no longer have the same
property. 
% \newgbt{very good!}
%As seen in Table~\ref{tab:result}, the performance becomes worse after we stack more convolutional layers. One possible reason is that, the context of language modeling is not a full sentence as in the work of~\cite{Kalchbrenner2014conv}, in which deep convolution can be interpreted as tree-based syntactic structure.

%Importantly, we struggled to try our networks with a very large setting (the result is not shown). The computation at large embeddings becomes expensive, and more importantly, the models overfit even to Europarl-NC, making the improvement negligible. Either very large dropout needs to be used, or the limitation of the models has been reached, which can be answered when higher computational power is available and more investigation is done. 

Regarding the comparison with a stronger LSTM, our models can perform competitively under the same embedding
dimension. However, the LSTM can be easily scaled using larger models,
as shown in~\newcite{zaremba2014recurrent}, which gives the best
known results to date. This is not an option for our model which heavily overfits with large 
hidden layers (around $1000$) even with very large dropout values. 

% \newgbt{added following, pls check.}


% \newgbt{I don't understand the following.}
%~ Since the feed forward structure normally
%~ requires a large hidden layer while the hidden layers of RNN models
%~ are shared across time steps, we end up using $1.5$ times more
%~ parameters than a LSTM/RNN with the same word embedding
%~ size. Specifically, our models with $k=256$ and a single convolutional
%~ block have approximately the same number of parameters as an LSTM
%~ with $H=400$.
%\newgbt{so? how does that affect the scaling of the model? why can't
%we do the same as them?}

% \gk{Don't think this discussion is adding anything to the message of the paper}
%In terms of the number of parameters, one of the main weaknesses of FF networks is that they may consume a lot more weights than recurrent networks, whose hidden weights are shared across time steps. In general, a feed forward network needs a large hidden layer to perform on par with an RNN~\cite{devlin2014fast}. Also the FFNN also requires a large matrix computation at the first layer to map the large context into the hidden layer. Our feed-forward baseline has 1.5X more weights than a RNN and LSTM (with Zaremba's implementation it no longer needs 4X more weights than RNN~\cite{zaremba2014recurrent}). However, the convolutional layers only add about 1-5\% to the number of weights.

%Condensed in one sentence above
%\paragraph{Kernel size effect.}	In all of our experiments, we started out with the kernel size of $3$ and stride of $1$ as inspired by the VGG network~\cite{simonyan2014very}. We observed that kernels with size 5 perform similarly (with stride 1 as well). When we increase the kernel size to (7 or 9), it is observed that the performance decreases slightly. Reasonably, with very large kernels, two adjacent windows have very similar content words (only one word as difference). However, further exploration with larger strides are not useful as well. 

% Already discussed in models, nothing related to the experimental results to be discuse=sed here.
%\paragraph{Batch Normalisation effect.} Batch normalisation is applied after each convolutional layer (2 batch norm layers would be used in the case of NIN networks). The main effect of batch normalisation in our experiments is regularisation. Concretely, it offers the same effect as dropout. Interestingly, batch normalisation is used only on the convolutional part of the network, while the fully connected layers are equipped with dropout. We do not use other regularisation methods such as L2 penalty. 
%\\

%~ Maybe put this to conclusion ?
To sum up, we have established that the results of our CNN model are
well above those of simple feed forward networks and recurrent neural
networks. While they are below state of the art LSTMs, they are able
to perform competitively with them for small and moderate-size
models. Scaling to larger sizes may be today the main roadblock 
for CNNs to reach the same performances
as large LSTMs in language modeling. % is\dots
