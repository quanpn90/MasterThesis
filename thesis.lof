\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Neural Language Model by Bengio et al\nobreakspace {}\cite {bengio2003neural}.}}{8}
\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple RNNLM\nobreakspace {}\cite {mikolov2010recurrent} predicting the sequence ``The cat is eating fish bone''.}}{15}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of baseline FFLM.}}{20}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Convolutional layer on top of the context matrix.}}{23}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Combining kernels with different sizes. We concatenate the outputs of 2 convolutional blocks with kernel size of $5$ and $3$ respectively.}}{24}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Some example phrases that have highest activations for 8 example kernels (each box), extracted from the validation set of the Penn Treebank. Model trained with 256 kernels for 256-dimension word vectors.}}{27}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.2}{\ignorespaces The distribution of positive weights over context positions, where 1 is the position closest to the predicted word.}}{28}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Perplexity change over position, by incrementally revealing the Mapping's weights corresponding to each position.}}{30}
\addvspace {10\p@ }
