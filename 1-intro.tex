Convolutional Neural Networks (CNNs) are the family of neural network
models that feature a type of layer known as the convolutional
layer. This layer can extract features by convolving a learnable filter (or
kernel) along different positions of a vectorial input.

CNNs have been successfully applied in Computer Vision in many
different tasks, including object recognition, scene parsing, action
recognition, among others~\cite{Gu:etal:2015}, but they have received less
attention in NLP. Mainly, they have been somewhat explored in
\emph{static classification tasks} where the model is provided with a
full linguistic unit as input (e.g.\ a sentence) and classes are
treated as independent of each other. Examples of this are sentence or
document classification for tasks such as Sentiment Analysis or Topic
Categorization \cite{Kalchbrenner2014conv,kim2014sentence}, sentence
matching
% \newgbt{what is sentence matching?}
\cite{hu2014convolutional}, and relation extraction
\cite{nguyen2015relation}. However, their application to
\emph{sequential prediction tasks}, where the input is construed to be
part of a sequence (for example, language modeling or POS tagging),
has been rather limited (with exceptions, such
as~\newcite{collobert2011natural}). The main contribution of this
paper is a systematic evaluation of CNNs in the context of a prominent
sequential prediction task, namely, language modeling.
% We show that
% CNNs achieve good results that, even if below the state of the art, but well
% above those of feed forward neural networks and even Recurrent Neural
% Networks (RNNs).

Statistical language models are a crucial component in many Natural
Language Processing applications such as Automatic Speech Recognition,
Machine Translation, and Information Retrieval. Here, we study the
problem under the standard formulation of learning to predict the
upcoming token given its previous context.
One successful approach to this problem relies on counting the number
of occurrences of $n$-grams while using smoothing and back-off
techniques to estimate the probability of an upcoming
word~\cite{kneser1995improved}. However, since each individual word is
treated independently of the others, $n$-gram models fail to capture
semantic relations between words. In contrast, neural network language
models~\cite{bengio2006neural} learn to predict the upcoming word
given the previous context while embedding the vocabulary in a
continuous space that can represent the similarity structure between
words. Both feed-forward~\cite{schwenk2007continuous} and recurrent
neural networks~\cite{mikolov2010recurrent} have been shown to
outperform $n$-gram
models~\cite{mikolov2010recurrent,le2011structured} in various setups.
These two types of neural networks make different architectural
decisions. Recurrent networks take one token at a time together with a
hidden ``memory'' vector as input and produce a prediction and an
updated hidden vector for the next time step. In contrast,
feed-forward language models take as input the last $n$ tokens, where
$n$ is a fixed window size, and use them jointly to predict the
upcoming word. %Particularly, the model proposed by
%\newcite{bengio2006neural} uses a concatenation of the word embeddings
%corresponding to the input and then projects them into a hidden layer,
%which can learn interactions between these words. Others have tried
%stacking more hidden layers producing deeper
%models~\cite{arisoy2012deep}, but this approach has given little
%benefit.

%Even though recurrent models hold today the state-of-the-art record,
%feed-forward language models have appealing properties that are worth
%further exploring. Specifically, since they receive as input a
%sequence of $n$ tokens, they could potentially infer relations back
%and forward \gbt{Why ``back and forward''?} \gk{It means that you can access words produced at any time (within the window)} between them more easily
%than a recurrent network that has to squeeze all the information in
%the previous context into the hidden layer, especially for long-range
%dependencies. \gbt{Is it true that they are easier to train / can
%  scale better? If so, say it. More generally, we need to improve the
%  motivation: We say they can infer relations more easily, but RNNs
%  still win. What is it that makes FFNNs a good option? In which
%  setups is that a preferrable option over RNNs?} \gk{They are not. The point here is that perhaps we are ignoring an interesting path, and with this work we improve over this line, even if we don't make it attractive enough to be preferable to the other} Therefore, in this
%study we decided to focus on studying how can we improve feed-forward
%neural networks with a better encoding of these relations.

%As an observation, the words in the context are often treated individually. While feed-forward models concatenate the word vectors into one single context vector which is the input of the subsequent hidden layers, the recurrent networks take into account one word per time step, and rely on powerful sequential models such as the Long-Short Term Memory~\cite{hochreiter1997long} to learn the temporal dependency for syntactic and grammar regularisation~\cite{zaremba2014recurrent}. On the other hand, the long context itself can be factorised into constituent $n$-grams, for example multi-word expressions. The global smoothing function can be enhanced if the networks can acquire such regional representation of the context. Our interest in this paper, therefore, will focus on feed-forward models with long context inputs. While extending the network with extra hidden layers have little benefit~\cite{arisoy2012deep} that even requires a discriminative pre-training process, we are interested in the use of temporal convolutional neural networks that are recently exploited in the field of Natural Language Processing. 

%More specifically, in this work we investigate the effect of using a
%\textbf{convolutional} layer~\cite{lecun1995convolutional} instead of fully connected layers to
%better capture interdependencies between adjacent words. Convolution is $\dots$.  
%Here, we initially train a carefully tuned feed-forward
%neural language model, which yields competitive performance in various
%corpora. We then inject a convolutional layer between the input and
%the projection to the hidden layer, producing a model that is comparable
%except for this modification. The experimental results indicate that
%using convolutional layers can improve 15-20\% perplexity compared to
%the solid baseline, and the CNN model can perform on par with the
%state-of-the-art models. Our analysis also shows that the
%convolutional kernels independently learned to focus on different
%linguistic patterns and that the model can take into account context
%information which is very far from the target.

%In this paper we define and explore CNN-based language models and
%compare them with both feed-forward and recurrent neural networks. Our
%results show a 13\% perplexity improvement of the CNN with respect to
%the feed-forward language model, while comparable or higher performance
%compared to similarly-sized recurrent models, and lower performance
%with respect to larger, state-of-the-art recurrent language
%models~(LSTMs as trained in~\newcite{zaremba2014recurrent}).
%
%Our second contribution is an analysis of the kind of information
%learned by the CNN, showing that the network learns to extract a
%combination of grammatical, semantic, and topical information from
%tokens of all across the input window, even those that are the
%farthest from the target.

In the thesis, we define and explore the feed-forward neural network language models a
ugmented with convolutional layers and compare them with original feed-forward and recurrent neural networks. Our
results show a 13\% perplexity improvement of the CNN with respect to
the feed-forward language model, while comparable or higher performance
compared to similarly-sized recurrent models, and lower performance
with respect to larger, state-of-the-art recurrent language
models~(LSTMs as trained in~\newcite{zaremba2014recurrent}).

Our second contribution is an analysis of the kind of information
learned by the CNN, showing that the network learns to extract a
combination of grammatical, semantic, and topical information from
tokens of all across the input window, even those that are the
farthest from the target.

In brief, this thesis is organised as follows. Chapter~\ref{c:review} is 
dedicated to provide background knowledge about the motivation, architecture and
progress of neural language models by covering the details of the original
feed-forward and recurrent language models with most techniques applied in 
state-of-the-art systems. In chapter~\ref{c:cnn}, we describe the proposed
model with the addition of convolutional layers. Subsequently, details about the 
experiments will be delivered in Chapter~\ref{c:exp}. Finally, the model analysis in which we  will be presented in Chapter~\ref{c:analysis}.



%In this work, we investigate the feed-forward neural language models with temporal convolutional neural networks applied on the context embeddings. The concept of this structure has already been introduced~\cite{collobert2011natural}, in which the network is equipped with a convolutional-pooling mechanism to find the features for multiple natural language processing tasks including language modeling. However, it is unknown if the convolution networks deliver any improvement over the feed-forward counterpart. For that purpose, we initially train a carefully tuned feed-forward neural language model, which yield competitive performance in various corpora. Subsequently, we enhance the baseline with an additional convolutional layer that convolves over the word embeddings. The network is setup so that: the layers between two models are almost identical except the convolutional layers, and we can effortlessly limit the hyperparameter space for network training. The experimental results indicate that using convolutional layers can improve 15-20\% perplexity compared to the solid baseline, and the CNN model can perform on par with the state-of-the-art models. Our analysis also shows that the convolutional kernels independently learned to focus on different linguistic patterns, and the model can take into account context words at very far from the target. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
