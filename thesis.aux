\relax 
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iv}}
\citation{bengio2003neural}
\citation{mikolov2010recurrent}
\citation{kneser1995improved,chen1999empirical,heafield2011kenlm,federico2008irstlm,ney1994structuring,witten1991zero}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:intro}{{1}{1}}
\citation{kingsley1932selective}
\citation{bengio2006neural}
\citation{bengio2006neural}
\citation{schwenk2007continuous,le2011structured,vaswani2013decoding,mikolov2010recurrent}
\citation{mikolov2010recurrent,mikolov2011extensions}
\citation{sag2002multiword}
\citation{Gu:etal:2015}
\citation{Kalchbrenner2014conv,kim2014sentence}
\citation{hu2014convolutional}
\citation{nguyen2015relation}
\citation{collobert2011natural}
\citation{collobert2011natural}
\citation{collobert2011natural}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:review}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Statistical Language Modeling}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Problem overview}{5}}
\newlabel{eq:lm1}{{2.2}{5}}
\newlabel{tab:examplePPL}{{2.1.2}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces An example of perplexity computation for the sentence ``The relationship between Obama and Netanyahu is not exactly friendly''.}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Evaluation Metrics}{6}}
\newlabel{eq:ppl}{{2.3}{6}}
\citation{Rosenfeld:2000}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}$N$-gram models}{7}}
\newlabel{eq:ngram}{{2.4}{7}}
\citation{kneser1995improved,chen1999empirical,heafield2011kenlm,federico2008irstlm,ney1994structuring,witten1991zero}
\citation{brown1992class,niesler1996variable}
\citation{chelba2000structured,filimonov2009joint}
\citation{kingsley1932selective}
\newlabel{eq:ngram2}{{2.5}{8}}
\citation{bengio2003neural,mikolov2010recurrent}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Network Language Models}{9}}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Feed-forward neural language models}{10}}
\@writefile{toc}{\contentsline {paragraph}{Input layer}{10}}
\citation{bengio2003neural}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Neural Language Model by Bengio et al\nobreakspace  {}\cite  {bengio2003neural}.}}{11}}
\newlabel{fig:neuralBengio}{{2.1}{11}}
\@writefile{toc}{\contentsline {paragraph}{Hidden layer}{11}}
\newlabel{eq:hidden1}{{2.7}{11}}
\newlabel{tab:notationFFNN}{{2.2.1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Notations for neural network layers.}}{12}}
\citation{zaremba2014recurrent,le2011structured,mikolov2010recurrent}
\citation{zeiler2012adadelta}
\citation{tieleman2012lecture}
\citation{le1990handwritten}
\newlabel{eq:activation}{{2.8}{13}}
\@writefile{toc}{\contentsline {paragraph}{Output layer}{13}}
\newlabel{eq:linearoutput}{{2.9}{13}}
\newlabel{eq:softmax}{{2.10}{13}}
\citation{rumelhart1985learning}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training method}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Optimisation Process}{14}}
\@writefile{toc}{\contentsline {paragraph}{Objective Function}{14}}
\newlabel{eq:objective}{{2.11}{14}}
\newlabel{eq:objective2}{{2.12}{15}}
\@writefile{toc}{\contentsline {paragraph}{Output layer}{15}}
\@writefile{toc}{\contentsline {paragraph}{Hidden layers}{15}}
\@writefile{toc}{\contentsline {paragraph}{Input Layer}{15}}
\newlabel{eq:hidden1}{{2.15}{15}}
\citation{schwenk2007continuous,le2011structured}
\newlabel{eq:dhidden}{{2.16}{16}}
\newlabel{eq:dactivation}{{2.17}{16}}
\@writefile{toc}{\contentsline {paragraph}{Parameter Update}{16}}
\newlabel{eq:sgd}{{2.18}{16}}
\citation{elman1990finding}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Recurrent neural language models}{17}}
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks}{17}}
\citation{mikolov2010recurrent}
\citation{elman1990finding}
\citation{mikolov2010recurrent}
\@writefile{toc}{\contentsline {paragraph}{Recurrent Language Models}{18}}
\newlabel{eq:rnnhidden}{{2.20}{18}}
\newlabel{eq:rnnoutput}{{2.21}{18}}
\newlabel{tab:notationRec}{{2.2.4}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Notations for recurrent neural network layers.}}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A simple RNNLM\nobreakspace  {}\cite  {mikolov2010recurrent} predicting the sequence ``The cat is eating fish bone''.}}{20}}
\newlabel{fig:SimpleRNN}{{2.2}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4.1}Training Recurrent Networks}{20}}
\@writefile{toc}{\contentsline {paragraph}{At each time step}{21}}
\newlabel{eq:drnn1}{{2.24}{21}}
\newlabel{eq:drnn2}{{2.26}{21}}
\citation{bengio1994learning,pascanu2013difficulty}
\citation{mikolov2011extensions,hai2012measuring}
\citation{pascanu2013difficulty}
\newlabel{eq:drnn3}{{2.27}{22}}
\@writefile{toc}{\contentsline {paragraph}{Back-propagation through time}{22}}
\@writefile{toc}{\contentsline {paragraph}{Problems with BPTT}{22}}
\citation{mikolov2014learning}
\citation{le2015simple}
\citation{martens2011learning}
\citation{hochreiter1997long}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces BPTT algorithm for ``vanilla'' RNNs}}{23}}
\newlabel{alg:bpttrnn}{{1}{23}}
\@writefile{toc}{\contentsline {paragraph}{Dealing with gradient vanishing}{23}}
\citation{bengio2013advances}
\citation{graves2005framewise}
\citation{sutskever2014sequence}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {paragraph}{LSTM Structure}{24}}
\citation{cho2014learning}
\citation{zaremba2015empirical}
\newlabel{eq:lstm1}{{2.30}{25}}
\newlabel{eq:lstm2}{{2.31}{25}}
\newlabel{eq:lstm3}{{2.32}{25}}
\@writefile{toc}{\contentsline {paragraph}{Training LSTM}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{25}}
\citation{waibel1989phoneme}
\citation{fukushima1980neocognitron,serre2007robust}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The convolutional layer}{26}}
\@writefile{toc}{\contentsline {paragraph}{Forward propagation}{26}}
\citation{krizhevsky2012imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Simple illustration for convolution. The input is a 2D image, the output is obtained by sliding the kernel through the image.}}{27}}
\newlabel{fig:conv_ops}{{2.3}{27}}
\newlabel{eq:convolution}{{2.33}{27}}
\@writefile{toc}{\contentsline {paragraph}{Backward propagation}{27}}
\newlabel{eq:dconv}{{2.34}{28}}
\newlabel{eq:dconv2}{{2.35}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hyper parameters for Convolutional Neural Network}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Convolution neural networks for NLP}{29}}
\citation{bengio2006neural}
\citation{srivastava2015highway}
\citation{hinton2012dropout}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Convolutional Neural Language Models}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:cnn}{{3}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Base FFLM}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of baseline FFLM.}}{31}}
\newlabel{fig:baseline}{{3.1}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}CNN and variants}{31}}
\citation{DBLP:journals/corr/IoffeS15}
\citation{lin2013network}
\citation{Kalchbrenner2014conv}
\citation{kim2014sentence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convolutional layer on top of the context matrix.}}{33}}
\newlabel{fig:simpleconv}{{3.2}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Combining kernels with different sizes. We concatenate the outputs of 2 convolutional blocks with kernel size of $5$ and $3$ respectively.}}{33}}
\newlabel{fig:combine}{{3.3}{33}}
\citation{mikolov2014learning}
\citation{bojar-EtAl:2015:WMT}
\citation{koehn2007moses}
\citation{le2011structured,sukhbaatar2015end,devlin2014fast}
\citation{tieleman2012lecture}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:exp}{{4}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiment details}{34}}
\citation{zaremba2014recurrent}
\citation{mikolov2014learning}
\citation{mikolov2014learning}
\citation{zaremba2014recurrent}
\citation{Kalchbrenner2014conv}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Results}{35}}
\citation{zaremba2014recurrent}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Results on Penn Treebank and Europarl-NC. Figure of merit is perplexity (lower is better). Legend: $k$: embedding size; \textit  {ker}: kernel size; \textit  {val}: results on validation data; \textit  {test}: results on test data; \textit  {\#p}: number of parameters; \textit  {hid}: size of hidden layers; \textit  {\#l}: number of layers.}}{37}}
\newlabel{tab:result}{{4.1}{37}}
\citation{krizhevsky2012imagenet}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Some example phrases that have highest activations for 8 example kernels (each box), extracted from the validation set of the Penn Treebank. Model trained with 256 kernels for 256-dimension word vectors.}}{38}}
\newlabel{fig:kernels}{{5.1}{38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model Analysis}{38}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:analysis}{{5}{38}}
\@writefile{toc}{\contentsline {paragraph}{Learned patterns}{38}}
\citation{hai2012measuring}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The distribution of positive weights over context positions, where 1 is the position closest to the predicted word.}}{39}}
\newlabel{fig:weight-dist}{{5.2}{39}}
\@writefile{toc}{\contentsline {paragraph}{Temporal information}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Perplexity change over position, by incrementally revealing the Mapping's weights corresponding to each position.}}{41}}
\newlabel{fig:position}{{5.3}{41}}
\citation{lecun1995convolutional}
\citation{karen2014vgg,kaiming2015resnet}
\citation{kim2014sentence}
\citation{Kalchbrenner2014conv}
\citation{hu2014convolutional}
\citation{nguyen2015relation}
\citation{shen2014latent}
\citation{collobert2011natural}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Related Works}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:related}{{6}{42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{44}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:conc}{{7}{44}}
\bibstyle{plainnat}
\bibdata{thesis}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Publications by Author}{46}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{bengio1994learning}{{1}{1994}{{Bengio et~al.}}{{Bengio, Simard, and Frasconi}}}
\bibcite{bengio2003neural}{{2}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Jauvin}}}
\bibcite{bengio2006neural}{{3}{2006}{{Bengio et~al.}}{{Bengio, Schwenk, Sen{\'e}cal, Morin, and Gauvain}}}
\bibcite{bengio2013advances}{{4}{2013}{{Bengio et~al.}}{{Bengio, Boulanger-Lewandowski, and Pascanu}}}
\bibcite{bojar-EtAl:2015:WMT}{{5}{2015}{{Bojar et~al.}}{{Bojar, Chatterjee, Federmann, Haddow, Huck, Hokamp, Koehn, Logacheva, Monz, Negri, Post, Scarton, Specia, and Turchi}}}
\bibcite{brown1992class}{{6}{1992}{{Brown et~al.}}{{Brown, Desouza, Mercer, Pietra, and Lai}}}
\bibcite{chelba2000structured}{{7}{2000}{{Chelba and Jelinek}}{{}}}
\bibcite{chen1999empirical}{{8}{1999}{{Chen and Goodman}}{{}}}
\bibcite{cho2014learning}{{9}{2014}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{collobert2011natural}{{10}{2011}{{Collobert et~al.}}{{Collobert, Weston, Bottou, Karlen, Kavukcuoglu, and Kuksa}}}
\bibcite{devlin2014fast}{{11}{2014}{{Devlin et~al.}}{{Devlin, Zbib, Huang, Lamar, Schwartz, and Makhoul}}}
\bibcite{elman1990finding}{{12}{1990}{{Elman}}{{}}}
\bibcite{federico2008irstlm}{{13}{2008}{{Federico et~al.}}{{Federico, Bertoldi, and Cettolo}}}
\bibcite{filimonov2009joint}{{14}{2009}{{Filimonov and Harper}}{{}}}
\bibcite{fukushima1980neocognitron}{{15}{1980}{{Fukushima}}{{}}}
\bibcite{graves2005framewise}{{16}{2005}{{Graves and Schmidhuber}}{{}}}
\bibcite{Gu:etal:2015}{{17}{2015}{{Gu et~al.}}{{Gu, Wang, Kuen, Ma, Shahroudy, Shuai, Liu, Wang, and Wang}}}
\bibcite{le2011structured}{{18}{2011}{{Hai~Son et~al.}}{{Hai~Son, Oparin, Allauzen, Gauvain, and Yvon}}}
\bibcite{hai2012measuring}{{19}{2012}{{Hai~Son et~al.}}{{Hai~Son, Allauzen, and Yvon}}}
\bibcite{kaiming2015resnet}{{20}{2015}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{heafield2011kenlm}{{21}{2011}{{Heafield}}{{}}}
\bibcite{hinton2012dropout}{{22}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{hochreiter1997long}{{23}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2014convolutional}{{24}{2014}{{Hu et~al.}}{{Hu, Lu, Li, and Chen}}}
\bibcite{DBLP:journals/corr/IoffeS15}{{25}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{Kalchbrenner2014conv}{{26}{2014}{{Kalchbrenner et~al.}}{{Kalchbrenner, Grefenstette, and Blunsom}}}
\bibcite{kim2014sentence}{{27}{2014}{{Kim}}{{}}}
\bibcite{kingsley1932selective}{{28}{1932}{{Kingsley}}{{}}}
\bibcite{kneser1995improved}{{29}{1995}{{Kneser and Ney}}{{}}}
\bibcite{koehn2007moses}{{30}{2007}{{Koehn et~al.}}{{Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, et~al.}}}
\bibcite{krizhevsky2012imagenet}{{31}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{le2015simple}{{32}{2015}{{Le et~al.}}{{Le, Jaitly, and Hinton}}}
\bibcite{le1990handwritten}{{33}{1990}{{Le~Cun et~al.}}{{Le~Cun, Denker, Henderson, Howard, Hubbard, and Jackel}}}
\bibcite{lecun1995convolutional}{{34}{1995}{{LeCun and Bengio}}{{}}}
\bibcite{lin2013network}{{35}{2013}{{Lin et~al.}}{{Lin, Chen, and Yan}}}
\bibcite{martens2011learning}{{36}{2011}{{Martens and Sutskever}}{{}}}
\bibcite{mikolov2010recurrent}{{37}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur}}}
\bibcite{mikolov2011extensions}{{38}{2011}{{Mikolov et~al.}}{{Mikolov, Kombrink, Burget, {\v {C}}ernock{\`y}, and Khudanpur}}}
\bibcite{mikolov2014learning}{{39}{2014}{{Mikolov et~al.}}{{Mikolov, Joulin, Chopra, Mathieu, and Ranzato}}}
\bibcite{ney1994structuring}{{40}{1994}{{Ney et~al.}}{{Ney, Essen, and Kneser}}}
\bibcite{nguyen2015relation}{{41}{2015}{{Nguyen and Grishman}}{{}}}
\bibcite{niesler1996variable}{{42}{1996}{{Niesler and Woodland}}{{}}}
\bibcite{pascanu2013difficulty}{{43}{2013}{{Pascanu et~al.}}{{Pascanu, Mikolov, and Bengio}}}
\bibcite{Rosenfeld:2000}{{44}{2000}{{Rosenfeld}}{{}}}
\bibcite{rumelhart1985learning}{{45}{1985}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{sag2002multiword}{{46}{2002}{{Sag et~al.}}{{Sag, Baldwin, Bond, Copestake, and Flickinger}}}
\bibcite{schwenk2007continuous}{{47}{2007}{{Schwenk}}{{}}}
\bibcite{serre2007robust}{{48}{2007}{{Serre et~al.}}{{Serre, Wolf, Bileschi, Riesenhuber, and Poggio}}}
\bibcite{shen2014latent}{{49}{2014}{{Shen et~al.}}{{Shen, He, Gao, Deng, and Mesnil}}}
\bibcite{karen2014vgg}{{50}{2014}{{Simonyan and Zisserman}}{{}}}
\bibcite{srivastava2015highway}{{51}{2015}{{Srivastava et~al.}}{{Srivastava, Greff, and Schmidhuber}}}
\bibcite{sukhbaatar2015end}{{52}{2015}{{Sukhbaatar et~al.}}{{Sukhbaatar, Weston, Fergus, et~al.}}}
\bibcite{sutskever2014sequence}{{53}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{tieleman2012lecture}{{54}{2012}{{Tieleman and Hinton}}{{}}}
\bibcite{vaswani2013decoding}{{55}{2013}{{Vaswani et~al.}}{{Vaswani, Zhao, Fossum, and Chiang}}}
\bibcite{waibel1989phoneme}{{56}{1989}{{Waibel et~al.}}{{Waibel, Hanazawa, Hinton, Shikano, and Lang}}}
\bibcite{witten1991zero}{{57}{1991}{{Witten and Bell}}{{}}}
\bibcite{zaremba2015empirical}{{58}{2015}{{Zaremba}}{{}}}
\bibcite{zaremba2014recurrent}{{59}{2014}{{Zaremba et~al.}}{{Zaremba, Sutskever, and Vinyals}}}
\bibcite{zeiler2012adadelta}{{60}{2012}{{Zeiler}}{{}}}
